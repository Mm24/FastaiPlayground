{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Fastai Data Block API together with PyTorch and Keras Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores possibility of using fastai data block and augmentation API, and use Keras to do all the rest. This is an experiment to try and mix/match the 2 frameworks. This assumes you have good familiarity with either frameworks.\n",
    "\n",
    "**Why do this?**\n",
    "\n",
    "Just to see how it will work. If you have already a custom Keras model, loss, and metrics and whole bunch of neat tricks done and thoroughly debugged, you may not ready to do a full rewrite. But you come across some a new dataset and a couple of neat data augmentations in fastai that you want to use, you can quickly test your idea out.\n",
    "\n",
    "Given this is a small experiment and may not scale, you have to extrapolate to your case at your own risk. It could be ok if you want to test out something quick, or for debugging, but it may be inefficient.\n",
    "\n",
    "\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "* Python 3.6 or above\n",
    "\n",
    "Recommended:\n",
    "* fastai 1.0.42 \n",
    "* keras 2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from pathlib import *\n",
    "from six.moves import urllib\n",
    "\n",
    "from fastai.vision import *\n",
    "from fastai.collab import *\n",
    "from fastai.tabular import *\n",
    "from fastai.metrics import error_rate\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from PIL import Image\n",
    "import numpy \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Common utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _describe(self):\n",
    "    print(f\"shape: {self.shape}\")\n",
    "    print(f\"dtyle: {self.dtype}\")\n",
    "    print(f\"min_val: {torch.min(self)}\")\n",
    "    print(f\"max_val: {torch.max(self)}\")\n",
    "    print(f\"type: {type(self)}\")\n",
    "\n",
    "torch.Tensor.describe = _describe\n",
    "\n",
    "# can't monkey patch numpy.array, don't hack too much, so do this instead:\n",
    "def np_desc(a):\n",
    "    if isinstance(a, (np.ndarray)):\n",
    "        print(f\"shape: {a.shape}\")\n",
    "        print(f\"dtyle: {a.dtype}\")\n",
    "        \n",
    "        print(f\"content: {a}\")    # this is ok, it is truncated nicely.\n",
    "        print(f\"type: {type(a)}\")\n",
    "    else:\n",
    "        print(\"Not a numpy.ndarray\")\n",
    "        \n",
    "# def ():\n",
    "#     (x.numpy()*255.).astype(np.uint8).transpose((1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/kelvinchan/.fastai/data/mnist_png/training'),\n",
       " PosixPath('/Users/kelvinchan/.fastai/data/mnist_png/testing')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use MNIST sample\n",
    "path = untar_data(URLs.MNIST); path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelvinchan/tmp/python3_env/deeplearning/lib/python3.7/site-packages/fastai/data_block.py:411: UserWarning: Your validation set is empty. Is this is by design, use `no_split()` \n",
      "                 or pass `ignore_empty=True` when labelling to remove this warning.\n",
      "  or pass `ignore_empty=True` when labelling to remove this warning.\"\"\")\n"
     ]
    }
   ],
   "source": [
    "tfms = get_transforms(do_flip=False)\n",
    "data = ImageDataBunch.from_folder(path, train='training', ds_tfms=tfms, size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#     y = (x.numpy()*255.).astype(np.uint8).transpose((1, 2, 0))\n",
    "#     img = PIL.Image.fromarray(y)\n",
    "#     img = img.filter(ImageFilter.GaussianBlur(radius=radius))\n",
    "#     x = np.array(img).transpose((2, 0, 1))\n",
    "    \n",
    "#     return torch.tensor(x/255., dtype=torch.float32)\n",
    "\n",
    "# type(x.numpy().transpose((0, 2, 3, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Know Thy training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "-----------\n",
      "X:\n",
      "shape: torch.Size([64, 3, 32, 32])\n",
      "dtyle: torch.float32\n",
      "min_val: 0.0\n",
      "max_val: 1.0\n",
      "type: <class 'torch.Tensor'>\n",
      "Y:\n",
      "shape: torch.Size([64])\n",
      "dtyle: torch.int64\n",
      "min_val: 0\n",
      "max_val: 9\n",
      "type: <class 'torch.Tensor'>\n",
      "====================================\n",
      "batch: 1\n",
      "-----------\n",
      "X:\n",
      "shape: torch.Size([64, 3, 32, 32])\n",
      "dtyle: torch.float32\n",
      "min_val: 0.0\n",
      "max_val: 1.0\n",
      "type: <class 'torch.Tensor'>\n",
      "Y:\n",
      "shape: torch.Size([64])\n",
      "dtyle: torch.int64\n",
      "min_val: 0\n",
      "max_val: 9\n",
      "type: <class 'torch.Tensor'>\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# take a quick look at X, and Y mini-batches\n",
    "for (k, (x, y)) in enumerate(data.train_dl):\n",
    "    print(f\"batch: {k}\")\n",
    "    print(\"-----------\")\n",
    "    print(\"X:\")\n",
    "    x.describe()\n",
    "    print(\"Y:\")\n",
    "    y.describe()\n",
    "    print(\"====================================\")\n",
    "    if k > 0: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note**: \n",
    "\n",
    "* Batch size is 64\n",
    "* Image size is 32x32\n",
    "* X is channel first (being torch tensor) and normalized between 0. and 1. \n",
    "* Y is of type Int64\n",
    "* Y is **NOT** one-hot encoded as max_val is 9 and shape is [64]\n",
    "* Visualize X by plt.imshow(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x133f6d080>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((x[0].numpy()*255.).astype(np.uint8).transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch training\n",
    "The following code mostly borrowed from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c55dfc508a92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/python3_env/deeplearning/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/python3_env/deeplearning/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for (k, (X, Y)) in enumerate(data.train_dl, 0):\n",
    "        # get the inputs\n",
    "#         inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if k % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, k + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow((images[1].numpy()).astype(np.uint8).transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_to_be_blur = x[0]; x_to_be_blur.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(26, 26, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))                \n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(), model.input, model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
